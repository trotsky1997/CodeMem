# CTR é¢„ä¼°æ¨¡å‹è®¾è®¡ for CodeMem

## æ¦‚è¿°

å€Ÿé‰´ç”µå•†æœç´¢çš„ CTRï¼ˆClick-Through Rateï¼‰é¢„ä¼°æ¨¡å‹ï¼Œé¢„æµ‹ç”¨æˆ·ä¼šé€‰æ‹©å“ªä¸ªæœç´¢ç»“æœã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šä¸æ˜¯ç®€å•çš„ç›¸å…³æ€§æ’åºï¼Œè€Œæ˜¯é¢„æµ‹"ç”¨æˆ·æœ€å¯èƒ½éœ€è¦å“ªä¸ªç»“æœ"ã€‚

## é—®é¢˜å®šä¹‰

### ç”µå•†åœºæ™¯

```
è¾“å…¥ï¼šç”¨æˆ· + æŸ¥è¯¢ + å•†å“
è¾“å‡ºï¼šç”¨æˆ·ç‚¹å‡»è¯¥å•†å“çš„æ¦‚ç‡ P(click | user, query, item)
ç›®æ ‡ï¼šæœ€å¤§åŒ–ç‚¹å‡»ç‡
```

### CodeMem åœºæ™¯

```
è¾“å…¥ï¼šç”¨æˆ·å†å² + æŸ¥è¯¢ + å¯¹è¯ç»“æœ
è¾“å‡ºï¼šç”¨æˆ·é€‰æ‹©è¯¥ç»“æœçš„æ¦‚ç‡ P(select | history, query, conversation)
ç›®æ ‡ï¼šæœ€å¤§åŒ–ç»“æœæœ‰ç”¨æ€§
```

### å…³é”®å·®å¼‚

| ç»´åº¦ | ç”µå•† | CodeMem |
|-----|------|---------|
| ç”¨æˆ·è¡Œä¸º | ç‚¹å‡»ã€è´­ä¹° | Follow-up æŸ¥è¯¢ã€å¼•ç”¨ |
| æ•°æ®é‡ | æµ·é‡ç”¨æˆ· | å•ç”¨æˆ· |
| åé¦ˆä¿¡å· | æ˜¾å¼ç‚¹å‡» | éšå¼è¡Œä¸º |
| å®æ—¶æ€§ | æ¯«ç§’çº§ | ç§’çº§å¯æ¥å— |

## å®šä¹‰"ç‚¹å‡»"çš„ç­‰ä»·è¡Œä¸º

åœ¨ CodeMem ä¸­ï¼Œä»€ä¹ˆè¡Œä¸ºè¡¨ç¤º"è¿™ä¸ªç»“æœæœ‰ç”¨"ï¼Ÿ

### 1. æ˜¾å¼å¼•ç”¨ï¼ˆå¼ºä¿¡å·ï¼‰

```python
# ç”¨æˆ·åœ¨ follow-up æŸ¥è¯¢ä¸­å¼•ç”¨äº†æŸä¸ªç»“æœ
"ç¬¬ä¸€ä¸ª"  # å¼•ç”¨æ’åç¬¬1çš„ç»“æœ
"é‚£æ®µä»£ç "  # å¼•ç”¨åŒ…å«ä»£ç çš„ç»“æœ
"é‚£æ¬¡å¯¹è¯"  # å¼•ç”¨æŸä¸ªä¼šè¯
```

**æƒé‡ï¼š1.0**ï¼ˆæœ€å¼ºä¿¡å·ï¼‰

### 2. è¯é¢˜å»¶ç»­ï¼ˆä¸­ç­‰ä¿¡å·ï¼‰

```python
# ç”¨æˆ·åœ¨åç»­æŸ¥è¯¢ä¸­ç»§ç»­è®¨è®ºç›¸å…³è¯é¢˜
æŸ¥è¯¢1: "Python å¼‚æ­¥ç¼–ç¨‹"
ç»“æœ: [å…³äº asyncio çš„å¯¹è¯]
æŸ¥è¯¢2: "asyncio.gather æ€ä¹ˆç”¨"  # è¯é¢˜å»¶ç»­

# è¯´æ˜ç¬¬ä¸€æ¬¡æœç´¢çš„ç»“æœæœ‰ç”¨
```

**æƒé‡ï¼š0.7**

### 3. ä¼šè¯å»¶ç»­ï¼ˆå¼±ä¿¡å·ï¼‰

```python
# ç”¨æˆ·åœ¨åŒä¸€ä¼šè¯ä¸­ç»§ç»­æé—®
# è¯´æ˜å½“å‰ä¸Šä¸‹æ–‡æœ‰ä»·å€¼
```

**æƒé‡ï¼š0.3**

### 4. è´Ÿåé¦ˆä¿¡å·

```python
# ç”¨æˆ·é‡å¤æœç´¢ç›¸åŒæˆ–ç›¸ä¼¼çš„æŸ¥è¯¢
æŸ¥è¯¢1: "Python å¼‚æ­¥"
æŸ¥è¯¢2: "Python å¼‚æ­¥"  # 5åˆ†é’Ÿå†…é‡å¤

# è¯´æ˜ç¬¬ä¸€æ¬¡æœç´¢æ²¡æ‰¾åˆ°ç­”æ¡ˆ
```

**æƒé‡ï¼š-0.5**

## ç‰¹å¾å·¥ç¨‹

### 1. æŸ¥è¯¢ç‰¹å¾ï¼ˆQuery Featuresï¼‰

```python
class QueryFeatures:
    """æŸ¥è¯¢ç›¸å…³ç‰¹å¾"""

    def extract(self, query: str) -> Dict[str, float]:
        return {
            # åŸºç¡€ç‰¹å¾
            'query_length': len(query),  # æŸ¥è¯¢é•¿åº¦
            'query_word_count': len(query.split()),  # è¯æ•°
            'has_code': 1.0 if '```' in query else 0.0,  # æ˜¯å¦åŒ…å«ä»£ç 

            # æŸ¥è¯¢ç±»å‹
            'is_how_question': 1.0 if any(w in query.lower() for w in ['å¦‚ä½•', 'how']) else 0.0,
            'is_why_question': 1.0 if any(w in query.lower() for w in ['ä¸ºä»€ä¹ˆ', 'why']) else 0.0,
            'is_what_question': 1.0 if any(w in query.lower() for w in ['ä»€ä¹ˆ', 'what']) else 0.0,

            # æŠ€æœ¯æ·±åº¦
            'tech_keyword_count': self._count_tech_keywords(query),
            'has_version_number': 1.0 if re.search(r'\d+\.\d+', query) else 0.0,

            # æ—¶é—´ç›¸å…³
            'has_time_expression': 1.0 if self._has_time_expr(query) else 0.0,
        }
```

### 2. å¯¹è¯ç‰¹å¾ï¼ˆConversation Featuresï¼‰

```python
class ConversationFeatures:
    """å¯¹è¯ç»“æœç›¸å…³ç‰¹å¾"""

    def extract(self, conversation: Dict) -> Dict[str, float]:
        return {
            # å¯¹è¯è´¨é‡
            'message_count': len(conversation['messages']),  # å¯¹è¯é•¿åº¦
            'avg_message_length': self._avg_length(conversation['messages']),
            'has_code_block': 1.0 if self._has_code(conversation) else 0.0,
            'code_block_count': self._count_code_blocks(conversation),

            # å¯¹è¯ç»“æ„
            'turn_count': self._count_turns(conversation),  # è½®æ¬¡
            'has_solution': 1.0 if self._detect_solution(conversation) else 0.0,
            'has_confirmation': 1.0 if self._has_confirmation(conversation) else 0.0,

            # æŠ€æœ¯å¯†åº¦
            'tech_keyword_density': self._tech_density(conversation),
            'unique_tech_keywords': self._unique_tech_keywords(conversation),

            # æ—¶é—´ç‰¹å¾
            'days_ago': (datetime.now() - conversation['timestamp']).days,
            'is_recent': 1.0 if self._is_recent(conversation, days=7) else 0.0,

            # ConversationRankï¼ˆPhase 6.1ï¼‰
            'conversation_rank': conversation.get('rank', 0.5),
        }
```

### 3. åŒ¹é…ç‰¹å¾ï¼ˆMatch Featuresï¼‰

```python
class MatchFeatures:
    """æŸ¥è¯¢ä¸å¯¹è¯çš„åŒ¹é…ç‰¹å¾"""

    def extract(self, query: str, conversation: Dict) -> Dict[str, float]:
        return {
            # æ–‡æœ¬åŒ¹é…
            'bm25_score': self._bm25_score(query, conversation),
            'exact_match_count': self._exact_matches(query, conversation),
            'fuzzy_match_score': self._fuzzy_match(query, conversation),

            # å…³é”®è¯åŒ¹é…
            'keyword_overlap': self._keyword_overlap(query, conversation),
            'tech_keyword_overlap': self._tech_keyword_overlap(query, conversation),

            # è¯­ä¹‰åŒ¹é…
            'query_in_title': 1.0 if query.lower() in conversation['first_message'].lower() else 0.0,
            'topic_match': 1.0 if self._topic_match(query, conversation) else 0.0,
        }
```

### 4. ç”¨æˆ·å†å²ç‰¹å¾ï¼ˆUser History Featuresï¼‰

```python
class UserHistoryFeatures:
    """ç”¨æˆ·å†å²è¡Œä¸ºç‰¹å¾"""

    def extract(self, user_history: Dict, conversation: Dict) -> Dict[str, float]:
        return {
            # è¯é¢˜åå¥½
            'user_topic_preference': self._topic_preference(user_history, conversation),
            'topic_frequency': self._topic_frequency(user_history, conversation['topic']),

            # æ—¶é—´åå¥½
            'user_prefers_recent': self._prefers_recent(user_history),
            'user_avg_result_age': self._avg_result_age(user_history),

            # å¯¹è¯ç±»å‹åå¥½
            'prefers_long_conversations': self._prefers_long(user_history),
            'prefers_code_heavy': self._prefers_code(user_history),

            # å¼•ç”¨å†å²
            'has_referenced_before': 1.0 if self._has_referenced(user_history, conversation) else 0.0,
            'reference_count': self._reference_count(user_history, conversation),
        }
```

### 5. ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆContext Featuresï¼‰

```python
class ContextFeatures:
    """å½“å‰ä¼šè¯ä¸Šä¸‹æ–‡ç‰¹å¾"""

    def extract(self, context: Dict, conversation: Dict) -> Dict[str, float]:
        return {
            # ä¼šè¯è¿ç»­æ€§
            'is_same_session': 1.0 if context.get('current_session') == conversation['session_id'] else 0.0,
            'is_recent_session': 1.0 if conversation['session_id'] in context.get('recent_sessions', []) else 0.0,

            # è¯é¢˜è¿ç»­æ€§
            'topic_continuity': self._topic_continuity(context, conversation),
            'keyword_continuity': self._keyword_continuity(context, conversation),

            # æŸ¥è¯¢å†å²
            'query_similarity_to_last': self._query_similarity(context, conversation),
            'is_follow_up': 1.0 if context.get('is_follow_up') else 0.0,
        }
```

### 6. ä½ç½®ç‰¹å¾ï¼ˆPosition Featuresï¼‰

```python
class PositionFeatures:
    """ç»“æœä½ç½®ç‰¹å¾ï¼ˆé‡è¦ï¼ï¼‰"""

    def extract(self, position: int) -> Dict[str, float]:
        return {
            # ä½ç½®åå·®ï¼ˆç”¨æˆ·å€¾å‘äºç‚¹å‡»å‰é¢çš„ç»“æœï¼‰
            'position': position,  # 1, 2, 3, ...
            'position_bias': 1.0 / math.log2(position + 1),  # DCG é£æ ¼çš„ä½ç½®åå·®

            # ä½ç½®åˆ†ç»„
            'is_top_1': 1.0 if position == 1 else 0.0,
            'is_top_3': 1.0 if position <= 3 else 0.0,
            'is_top_5': 1.0 if position <= 5 else 0.0,
        }
```

## CTR é¢„ä¼°æ¨¡å‹

### æ–¹æ¡ˆ 1ï¼šLogistic Regressionï¼ˆç®€å•å¿«é€Ÿï¼‰â­â­â­â­â­

```python
import numpy as np
from typing import List, Dict

class LogisticRegressionCTR:
    """
    é€»è¾‘å›å½’ CTR é¢„ä¼°æ¨¡å‹

    ä¼˜ç‚¹ï¼š
    - ç®€å•ã€å¿«é€Ÿã€å¯è§£é‡Š
    - é€‚åˆå°æ•°æ®é‡
    - ç‰¹å¾æƒé‡æ¸…æ™°

    ç¼ºç‚¹ï¼š
    - æ— æ³•æ•æ‰ç‰¹å¾äº¤å‰
    """

    def __init__(self):
        self.weights = None
        self.feature_names = []

    def extract_features(self, query: str, conversation: Dict,
                        user_history: Dict, context: Dict, position: int) -> np.ndarray:
        """æå–æ‰€æœ‰ç‰¹å¾"""
        features = {}

        # 1. æŸ¥è¯¢ç‰¹å¾
        features.update(QueryFeatures().extract(query))

        # 2. å¯¹è¯ç‰¹å¾
        features.update(ConversationFeatures().extract(conversation))

        # 3. åŒ¹é…ç‰¹å¾
        features.update(MatchFeatures().extract(query, conversation))

        # 4. ç”¨æˆ·å†å²ç‰¹å¾
        features.update(UserHistoryFeatures().extract(user_history, conversation))

        # 5. ä¸Šä¸‹æ–‡ç‰¹å¾
        features.update(ContextFeatures().extract(context, conversation))

        # 6. ä½ç½®ç‰¹å¾
        features.update(PositionFeatures().extract(position))

        # è½¬æ¢ä¸ºå‘é‡
        self.feature_names = sorted(features.keys())
        feature_vector = np.array([features[name] for name in self.feature_names])

        return feature_vector

    def predict(self, features: np.ndarray) -> float:
        """é¢„æµ‹ CTR"""
        if self.weights is None:
            # åˆå§‹æƒé‡ï¼ˆå¯å‘å¼ï¼‰
            self.weights = self._initialize_weights()

        # Logistic function: P = 1 / (1 + exp(-wÂ·x))
        logit = np.dot(self.weights, features)
        ctr = 1.0 / (1.0 + np.exp(-logit))

        return ctr

    def _initialize_weights(self) -> np.ndarray:
        """
        åˆå§‹åŒ–æƒé‡ï¼ˆåŸºäºé¢†åŸŸçŸ¥è¯†ï¼‰

        åœ¨æ²¡æœ‰è®­ç»ƒæ•°æ®å‰ï¼Œä½¿ç”¨å¯å‘å¼æƒé‡
        """
        weights = {}

        # åŒ¹é…ç‰¹å¾ï¼ˆæœ€é‡è¦ï¼‰
        weights['bm25_score'] = 2.0
        weights['keyword_overlap'] = 1.5
        weights['tech_keyword_overlap'] = 1.5

        # ConversationRank
        weights['conversation_rank'] = 1.5

        # å¯¹è¯è´¨é‡
        weights['has_solution'] = 1.0
        weights['has_code_block'] = 0.8
        weights['message_count'] = 0.3

        # æ—¶é—´ç‰¹å¾
        weights['is_recent'] = 0.5
        weights['days_ago'] = -0.01  # è´Ÿæƒé‡ï¼šè¶Šä¹…è¶Šä¸ç›¸å…³

        # ä¸Šä¸‹æ–‡
        weights['is_same_session'] = 1.2
        weights['topic_continuity'] = 0.8
        weights['is_follow_up'] = 0.5

        # ä½ç½®åå·®
        weights['position_bias'] = 0.5
        weights['is_top_1'] = 0.3

        # ç”¨æˆ·åå¥½
        weights['user_topic_preference'] = 0.8
        weights['has_referenced_before'] = 1.0

        # é»˜è®¤æƒé‡
        for name in self.feature_names:
            if name not in weights:
                weights[name] = 0.1

        return np.array([weights.get(name, 0.1) for name in self.feature_names])

    def train(self, training_data: List[Dict]):
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰

        training_data: [
            {
                'query': '...',
                'conversation': {...},
                'user_history': {...},
                'context': {...},
                'position': 1,
                'label': 1.0  # 1=é€‰æ‹©äº†è¿™ä¸ªç»“æœ, 0=æ²¡é€‰æ‹©
            },
            ...
        ]
        """
        if not training_data:
            return

        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        X = []
        y = []
        for sample in training_data:
            features = self.extract_features(
                sample['query'],
                sample['conversation'],
                sample['user_history'],
                sample['context'],
                sample['position']
            )
            X.append(features)
            y.append(sample['label'])

        X = np.array(X)
        y = np.array(y)

        # æ¢¯åº¦ä¸‹é™
        learning_rate = 0.01
        epochs = 100

        if self.weights is None:
            self.weights = self._initialize_weights()

        for epoch in range(epochs):
            # é¢„æµ‹
            predictions = 1.0 / (1.0 + np.exp(-np.dot(X, self.weights)))

            # è®¡ç®—æ¢¯åº¦
            gradient = np.dot(X.T, (predictions - y)) / len(y)

            # æ›´æ–°æƒé‡
            self.weights -= learning_rate * gradient

            # è®¡ç®—æŸå¤±ï¼ˆå¯é€‰ï¼‰
            if epoch % 10 == 0:
                loss = -np.mean(y * np.log(predictions + 1e-10) +
                               (1 - y) * np.log(1 - predictions + 1e-10))
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
```

### æ–¹æ¡ˆ 2ï¼šFactorization Machinesï¼ˆç‰¹å¾äº¤å‰ï¼‰â­â­â­â­

```python
class FactorizationMachineCTR:
    """
    å› å­åˆ†è§£æœº CTR é¢„ä¼°æ¨¡å‹

    ä¼˜ç‚¹ï¼š
    - è‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤å‰
    - é€‚åˆç¨€ç–ç‰¹å¾
    - æ¯” LR æ›´å¼ºå¤§

    ç¼ºç‚¹ï¼š
    - è®¡ç®—å¤æ‚åº¦æ›´é«˜
    - éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®
    """

    def __init__(self, n_factors: int = 10):
        self.n_factors = n_factors
        self.w0 = 0.0  # å…¨å±€åç½®
        self.w = None  # ä¸€é˜¶æƒé‡
        self.V = None  # äºŒé˜¶äº¤å‰çŸ©é˜µ

    def predict(self, features: np.ndarray) -> float:
        """
        FM é¢„æµ‹å…¬å¼ï¼š
        y = w0 + Î£(wiÂ·xi) + Î£(Î£(<vi,vj>Â·xiÂ·xj))
        """
        if self.w is None:
            self._initialize(len(features))

        # ä¸€é˜¶é¡¹
        linear = self.w0 + np.dot(self.w, features)

        # äºŒé˜¶äº¤å‰é¡¹ï¼ˆä¼˜åŒ–è®¡ç®—ï¼‰
        interaction = 0.0
        for f in range(self.n_factors):
            sum_square = np.sum(self.V[:, f] * features) ** 2
            square_sum = np.sum((self.V[:, f] ** 2) * (features ** 2))
            interaction += sum_square - square_sum

        interaction *= 0.5

        # Sigmoid
        logit = linear + interaction
        ctr = 1.0 / (1.0 + np.exp(-logit))

        return ctr

    def _initialize(self, n_features: int):
        """åˆå§‹åŒ–å‚æ•°"""
        self.w = np.random.randn(n_features) * 0.01
        self.V = np.random.randn(n_features, self.n_factors) * 0.01
```

### æ–¹æ¡ˆ 3ï¼šè½»é‡çº§ç¥ç»ç½‘ç»œï¼ˆæ·±åº¦å­¦ä¹ ï¼‰â­â­â­

```python
class DeepCTR:
    """
    è½»é‡çº§æ·±åº¦ CTR æ¨¡å‹ï¼ˆç±»ä¼¼ Wide & Deepï¼‰

    ä¼˜ç‚¹ï¼š
    - å¼ºå¤§çš„éçº¿æ€§æ‹Ÿåˆèƒ½åŠ›
    - å¯ä»¥å­¦ä¹ å¤æ‚æ¨¡å¼

    ç¼ºç‚¹ï¼š
    - éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®
    - è®¡ç®—å¼€é”€å¤§
    - å¯è§£é‡Šæ€§å·®

    æ³¨æ„ï¼šCodeMem æ•°æ®é‡æœ‰é™ï¼Œä¸æ¨èä½¿ç”¨
    """
    pass  # æš‚ä¸å®ç°
```

## æ”¶é›†è®­ç»ƒæ•°æ®

### 1. éšå¼åé¦ˆæ”¶é›†

```python
class FeedbackCollector:
    """æ”¶é›†ç”¨æˆ·éšå¼åé¦ˆ"""

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.feedback_log = []

    async def log_search(self, query: str, results: List[Dict], context: Dict):
        """è®°å½•æœç´¢è¡Œä¸º"""
        search_id = str(uuid.uuid4())

        await self._save_search_log({
            'search_id': search_id,
            'query': query,
            'results': [r['session_id'] for r in results],
            'timestamp': datetime.now().isoformat(),
            'context': context
        })

        return search_id

    async def log_selection(self, search_id: str, selected_position: int,
                           selection_type: str, confidence: float):
        """
        è®°å½•ç”¨æˆ·é€‰æ‹©è¡Œä¸º

        selection_type:
        - 'explicit_reference': æ˜¾å¼å¼•ç”¨ï¼ˆ"ç¬¬ä¸€ä¸ª"ï¼‰
        - 'topic_continuation': è¯é¢˜å»¶ç»­
        - 'session_continuation': ä¼šè¯å»¶ç»­
        """
        await self._save_feedback({
            'search_id': search_id,
            'selected_position': selected_position,
            'selection_type': selection_type,
            'confidence': confidence,
            'timestamp': datetime.now().isoformat()
        })

    async def log_negative_feedback(self, search_id: str, reason: str):
        """
        è®°å½•è´Ÿåé¦ˆ

        reason:
        - 'repeated_query': é‡å¤æŸ¥è¯¢
        - 'no_follow_up': æ²¡æœ‰åç»­è¡Œä¸º
        """
        await self._save_feedback({
            'search_id': search_id,
            'selected_position': -1,
            'selection_type': 'negative',
            'reason': reason,
            'timestamp': datetime.now().isoformat()
        })
```

### 2. è‡ªåŠ¨æ ‡æ³¨è®­ç»ƒæ•°æ®

```python
async def generate_training_data(db_path: str, days: int = 30) -> List[Dict]:
    """
    ä»å†å²æœç´¢æ—¥å¿—ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®

    æ ‡æ³¨è§„åˆ™ï¼š
    1. å¦‚æœç”¨æˆ·å¼•ç”¨äº†æŸä¸ªç»“æœ â†’ label=1.0
    2. å¦‚æœç”¨æˆ·ç»§ç»­è®¨è®ºç›¸å…³è¯é¢˜ â†’ label=0.7
    3. å¦‚æœç”¨æˆ·åœ¨åŒä¸€ä¼šè¯ç»§ç»­ â†’ label=0.3
    4. å¦‚æœç”¨æˆ·é‡å¤æœç´¢ â†’ æ‰€æœ‰ç»“æœ label=0.0
    5. å…¶ä»–æœªé€‰æ‹©çš„ç»“æœ â†’ label=0.0
    """
    search_logs = await load_search_logs(db_path, days)
    feedback_logs = await load_feedback_logs(db_path, days)

    training_data = []

    for search in search_logs:
        # æŸ¥æ‰¾å¯¹åº”çš„åé¦ˆ
        feedback = find_feedback(search['search_id'], feedback_logs)

        for position, result in enumerate(search['results'], 1):
            label = 0.0

            if feedback:
                if feedback['selected_position'] == position:
                    # ç”¨æˆ·é€‰æ‹©äº†è¿™ä¸ªç»“æœ
                    if feedback['selection_type'] == 'explicit_reference':
                        label = 1.0
                    elif feedback['selection_type'] == 'topic_continuation':
                        label = 0.7
                    elif feedback['selection_type'] == 'session_continuation':
                        label = 0.3

            training_data.append({
                'query': search['query'],
                'conversation': result,
                'user_history': search['context'].get('user_history', {}),
                'context': search['context'],
                'position': position,
                'label': label
            })

    return training_data
```

## åœ¨çº¿å­¦ä¹ ï¼ˆOnline Learningï¼‰

```python
class OnlineCTRModel:
    """
    åœ¨çº¿å­¦ä¹  CTR æ¨¡å‹

    ç‰¹ç‚¹ï¼š
    - å®æ—¶æ›´æ–°æ¨¡å‹
    - é€‚åº”ç”¨æˆ·è¡Œä¸ºå˜åŒ–
    - æ— éœ€ç¦»çº¿è®­ç»ƒ
    """

    def __init__(self):
        self.model = LogisticRegressionCTR()
        self.feedback_buffer = []
        self.update_threshold = 10  # ç´¯ç§¯10ä¸ªæ ·æœ¬åæ›´æ–°

    async def predict_and_rank(self, query: str, candidates: List[Dict],
                               user_history: Dict, context: Dict) -> List[Dict]:
        """é¢„æµ‹ CTR å¹¶é‡æ–°æ’åº"""
        for position, candidate in enumerate(candidates, 1):
            features = self.model.extract_features(
                query, candidate, user_history, context, position
            )
            candidate['predicted_ctr'] = self.model.predict(features)

        # æŒ‰é¢„æµ‹ CTR æ’åº
        candidates.sort(key=lambda x: x['predicted_ctr'], reverse=True)

        return candidates

    async def update_with_feedback(self, feedback: Dict):
        """æ ¹æ®åé¦ˆæ›´æ–°æ¨¡å‹"""
        self.feedback_buffer.append(feedback)

        # ç´¯ç§¯è¶³å¤Ÿæ ·æœ¬åæ›´æ–°
        if len(self.feedback_buffer) >= self.update_threshold:
            self.model.train(self.feedback_buffer)
            self.feedback_buffer = []  # æ¸…ç©ºç¼“å†²åŒº
```

## è¯„ä¼°æŒ‡æ ‡

### 1. ç¦»çº¿æŒ‡æ ‡

```python
def evaluate_offline(model, test_data: List[Dict]) -> Dict[str, float]:
    """ç¦»çº¿è¯„ä¼°"""
    predictions = []
    labels = []

    for sample in test_data:
        features = model.extract_features(
            sample['query'],
            sample['conversation'],
            sample['user_history'],
            sample['context'],
            sample['position']
        )
        pred = model.predict(features)
        predictions.append(pred)
        labels.append(sample['label'])

    return {
        'auc': calculate_auc(labels, predictions),  # AUC
        'logloss': calculate_logloss(labels, predictions),  # Log Loss
        'accuracy': calculate_accuracy(labels, predictions),  # å‡†ç¡®ç‡
    }
```

### 2. åœ¨çº¿æŒ‡æ ‡

```python
def evaluate_online(search_logs: List[Dict]) -> Dict[str, float]:
    """åœ¨çº¿è¯„ä¼°ï¼ˆæ›´é‡è¦ï¼‰"""
    return {
        # ç‚¹å‡»ç‡ï¼ˆé€‰æ‹©ç‡ï¼‰
        'ctr': calculate_ctr(search_logs),

        # å¹³å‡é€‰æ‹©ä½ç½®ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        'mean_selected_position': calculate_mean_position(search_logs),

        # Top-3 å‘½ä¸­ç‡
        'hit_rate_at_3': calculate_hit_rate(search_logs, k=3),

        # MRR (Mean Reciprocal Rank)
        'mrr': calculate_mrr(search_logs),

        # NDCG (Normalized Discounted Cumulative Gain)
        'ndcg': calculate_ndcg(search_logs),
    }
```

## å®ç°è·¯çº¿å›¾

### Phase 6.1: ç‰¹å¾å·¥ç¨‹ (v1.1.0) â­â­â­â­â­

- [ ] å®ç° 6 ç±»ç‰¹å¾æå–å™¨
- [ ] å®ç°åé¦ˆæ”¶é›†ç³»ç»Ÿ
- [ ] å®ç°è®­ç»ƒæ•°æ®è‡ªåŠ¨æ ‡æ³¨

### Phase 6.2: LR æ¨¡å‹ (v1.2.0) â­â­â­â­â­

- [ ] å®ç° Logistic Regression CTR æ¨¡å‹
- [ ] å¯å‘å¼æƒé‡åˆå§‹åŒ–
- [ ] é›†æˆåˆ° `memory.query` å·¥å…·

### Phase 6.3: åœ¨çº¿å­¦ä¹  (v1.3.0) â­â­â­â­

- [ ] å®ç°åœ¨çº¿å­¦ä¹ æœºåˆ¶
- [ ] å®æ—¶æ¨¡å‹æ›´æ–°
- [ ] A/B æµ‹è¯•æ¡†æ¶

### Phase 6.4: é«˜çº§æ¨¡å‹ (v1.4.0) â­â­â­

- [ ] å®ç° FM æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
- [ ] ç‰¹å¾äº¤å‰ä¼˜åŒ–
- [ ] æ¨¡å‹é›†æˆ

## å†·å¯åŠ¨ç­–ç•¥ï¼šä»ä½ç½®åæ¨ CTR â­â­â­â­â­

### æ ¸å¿ƒæ€æƒ³

**é—®é¢˜**ï¼šæ²¡æœ‰çœŸå®ç”¨æˆ·åé¦ˆï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šä» BM25 çš„åŸå§‹æ’åºåæ¨ CTRï¼Œä½œä¸ºåˆå§‹è®­ç»ƒæ•°æ®ã€‚

**å‡è®¾**ï¼šBM25 æ’åºæœ‰ä¸€å®šåˆç†æ€§ï¼Œæ’åœ¨å‰é¢çš„ç»“æœæ›´å¯èƒ½è¢«ç”¨æˆ·é€‰æ‹©ã€‚

### ä½ç½®åå·®æ¨¡å‹

```python
def position_to_pseudo_ctr(position: int) -> float:
    """
    ä»ä½ç½®åæ¨ä¼ª CTR

    åŸºäºç»éªŒå…¬å¼ï¼ˆç±»ä¼¼ Google çš„ç‚¹å‡»æ¨¡å‹ï¼‰ï¼š
    CTR(position) = 1 / log2(position + 1)

    ä½ç½® 1: 1.0 / log2(2) = 1.0
    ä½ç½® 2: 1.0 / log2(3) = 0.63
    ä½ç½® 3: 1.0 / log2(4) = 0.50
    ä½ç½® 4: 1.0 / log2(5) = 0.43
    ä½ç½® 5: 1.0 / log2(6) = 0.39
    ...
    """
    return 1.0 / math.log2(position + 1)


def generate_pseudo_training_data(search_history: List[Dict]) -> List[Dict]:
    """
    ä»å†å²æœç´¢ç»“æœç”Ÿæˆä¼ªè®­ç»ƒæ•°æ®

    è¾“å…¥ï¼šå†å²ä¸Šæ‰€æœ‰çš„æœç´¢è®°å½•ï¼ˆå¸¦ BM25 æ’åºï¼‰
    è¾“å‡ºï¼šå¸¦ä¼ªæ ‡ç­¾çš„è®­ç»ƒæ•°æ®
    """
    training_data = []

    for search in search_history:
        query = search['query']
        results = search['results']  # BM25 æ’åºçš„ç»“æœ
        context = search['context']

        for position, result in enumerate(results, 1):
            # ä»ä½ç½®åæ¨ä¼ª CTR
            pseudo_ctr = position_to_pseudo_ctr(position)

            # è½¬æ¢ä¸ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            # æ–¹æ¡ˆ1ï¼šç›´æ¥ç”¨ pseudo_ctr ä½œä¸ºè½¯æ ‡ç­¾
            label = pseudo_ctr

            # æ–¹æ¡ˆ2ï¼šè½¬æ¢ä¸ºç¡¬æ ‡ç­¾ï¼ˆ0/1ï¼‰
            # label = 1.0 if position <= 3 else 0.0

            training_data.append({
                'query': query,
                'conversation': result,
                'user_history': context.get('user_history', {}),
                'context': context,
                'position': position,
                'label': label,
                'is_pseudo': True  # æ ‡è®°ä¸ºä¼ªæ ‡ç­¾
            })

    return training_data
```

### ä¼˜ç‚¹ä¸ç¼ºç‚¹

#### âœ… ä¼˜ç‚¹

1. **ç«‹å³å¯ç”¨** - ä¸éœ€è¦ç­‰å¾…çœŸå®åé¦ˆ
2. **æ•°æ®é‡å¤§** - å†å²ä¸Šæ‰€æœ‰æœç´¢éƒ½å¯ä»¥ç”¨
3. **å†·å¯åŠ¨å‹å¥½** - æ–°ç”¨æˆ·ä¹Ÿèƒ½æœ‰åˆå§‹æ¨¡å‹
4. **å¿«é€Ÿè¿­ä»£** - å¯ä»¥ç«‹å³å¼€å§‹è®­ç»ƒå’Œæµ‹è¯•

#### âš ï¸ ç¼ºç‚¹

1. **å‡è®¾ BM25 æ˜¯å¯¹çš„** - ä½†å®é™…ä¸Š BM25 å¯èƒ½ä¸å‡†ç¡®
2. **å¾ªç¯ä¾èµ–** - ç”¨ BM25 è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹å­¦åˆ°çš„å°±æ˜¯ BM25 çš„æ¨¡å¼
3. **ä½ç½®åå·®** - ç”¨æˆ·å€¾å‘ç‚¹å‡»å‰é¢çš„ç»“æœï¼Œä¸ä»£è¡¨å‰é¢çš„ç»“æœå°±ä¸€å®šæ›´å¥½

### è§£å†³æ–¹æ¡ˆï¼šæ··åˆè®­ç»ƒç­–ç•¥ â­â­â­â­â­

```python
class HybridTrainingStrategy:
    """
    æ··åˆè®­ç»ƒç­–ç•¥ï¼šä¼ªæ ‡ç­¾ + çœŸå®åé¦ˆ

    é˜¶æ®µ1ï¼šç”¨ä¼ªæ ‡ç­¾è®­ç»ƒåˆå§‹æ¨¡å‹
    é˜¶æ®µ2ï¼šéƒ¨ç½²æ¨¡å‹ï¼Œæ”¶é›†çœŸå®åé¦ˆ
    é˜¶æ®µ3ï¼šç”¨çœŸå®åé¦ˆé€æ­¥æ›¿æ¢ä¼ªæ ‡ç­¾
    é˜¶æ®µ4ï¼šæŒç»­åœ¨çº¿å­¦ä¹ 
    """

    def __init__(self):
        self.pseudo_data = []  # ä¼ªæ ‡ç­¾æ•°æ®
        self.real_data = []    # çœŸå®åé¦ˆæ•°æ®
        self.model = LogisticRegressionCTR()

    async def initialize(self, search_history: List[Dict]):
        """é˜¶æ®µ1ï¼šç”¨ä¼ªæ ‡ç­¾è®­ç»ƒåˆå§‹æ¨¡å‹"""
        print("ğŸ”„ ç”Ÿæˆä¼ªè®­ç»ƒæ•°æ®...")
        self.pseudo_data = generate_pseudo_training_data(search_history)

        print(f"âœ… ç”Ÿæˆ {len(self.pseudo_data)} æ¡ä¼ªè®­ç»ƒæ•°æ®")
        print("ğŸ”„ è®­ç»ƒåˆå§‹æ¨¡å‹...")

        self.model.train(self.pseudo_data)
        print("âœ… åˆå§‹æ¨¡å‹è®­ç»ƒå®Œæˆ")

    async def collect_real_feedback(self, feedback: Dict):
        """é˜¶æ®µ2ï¼šæ”¶é›†çœŸå®åé¦ˆ"""
        self.real_data.append(feedback)

        # çœŸå®æ•°æ®è¾¾åˆ°ä¸€å®šé‡åï¼Œå¼€å§‹æ··åˆè®­ç»ƒ
        if len(self.real_data) >= 10:
            await self.hybrid_train()

    async def hybrid_train(self):
        """é˜¶æ®µ3ï¼šæ··åˆè®­ç»ƒï¼ˆä¼ªæ ‡ç­¾ + çœŸå®åé¦ˆï¼‰"""

        # è®¡ç®—çœŸå®æ•°æ®çš„æƒé‡ï¼ˆéšç€çœŸå®æ•°æ®å¢å¤šï¼Œæƒé‡å¢åŠ ï¼‰
        real_data_ratio = len(self.real_data) / (len(self.real_data) + len(self.pseudo_data))
        real_weight = min(real_data_ratio * 2, 1.0)  # æœ€å¤šåˆ°1.0
        pseudo_weight = 1.0 - real_weight

        print(f"ğŸ”„ æ··åˆè®­ç»ƒï¼šçœŸå®æ•°æ®æƒé‡={real_weight:.2f}, ä¼ªæ•°æ®æƒé‡={pseudo_weight:.2f}")

        # åŠ æƒæ··åˆè®­ç»ƒæ•°æ®
        training_data = []

        # æ·»åŠ çœŸå®æ•°æ®ï¼ˆé«˜æƒé‡ï¼‰
        for sample in self.real_data:
            sample['weight'] = real_weight
            training_data.append(sample)

        # æ·»åŠ ä¼ªæ•°æ®ï¼ˆä½æƒé‡ï¼Œä¸”éšç€çœŸå®æ•°æ®å¢å¤šè€Œé™ä½ï¼‰
        sample_size = min(len(self.pseudo_data), len(self.real_data) * 5)  # æœ€å¤š5å€
        sampled_pseudo = random.sample(self.pseudo_data, sample_size)

        for sample in sampled_pseudo:
            sample['weight'] = pseudo_weight
            training_data.append(sample)

        # è®­ç»ƒæ¨¡å‹
        self.model.train_weighted(training_data)

        print(f"âœ… æ··åˆè®­ç»ƒå®Œæˆï¼š{len(self.real_data)} çœŸå® + {sample_size} ä¼ªæ ‡ç­¾")

    async def phase_out_pseudo_data(self):
        """é˜¶æ®µ4ï¼šé€æ­¥æ·˜æ±°ä¼ªæ•°æ®"""

        # å½“çœŸå®æ•°æ®è¶³å¤Ÿå¤šæ—¶ï¼ˆä¾‹å¦‚ > 100ï¼‰ï¼Œå®Œå…¨åœç”¨ä¼ªæ•°æ®
        if len(self.real_data) > 100:
            print("âœ… çœŸå®æ•°æ®å……è¶³ï¼Œåœç”¨ä¼ªæ ‡ç­¾æ•°æ®")
            self.pseudo_data = []
            self.model.train(self.real_data)
```

### å»åæŠ€æœ¯ï¼ˆé«˜çº§ï¼‰

```python
def inverse_propensity_scoring(position: int) -> float:
    """
    é€†å€¾å‘å¾—åˆ†ï¼ˆInverse Propensity Scoringï¼‰

    ç”¨äºå»é™¤ä½ç½®åå·®ï¼š
    - ä½ç½®é å‰çš„ç»“æœï¼Œå³ä½¿è´¨é‡ä¸€èˆ¬ï¼Œä¹Ÿå®¹æ˜“è¢«ç‚¹å‡»
    - ä½ç½®é åçš„ç»“æœï¼Œå³ä½¿è´¨é‡å¾ˆå¥½ï¼Œä¹Ÿä¸å®¹æ˜“è¢«ç‚¹å‡»

    è§£å†³æ–¹æ¡ˆï¼šç»™ä½ç½®é åçš„æ ·æœ¬æ›´é«˜çš„æƒé‡
    """
    # ä½ç½®åå·®ï¼ˆç”¨æˆ·ç‚¹å‡»è¯¥ä½ç½®çš„å€¾å‘ï¼‰
    propensity = 1.0 / math.log2(position + 1)

    # é€†å€¾å‘å¾—åˆ†ï¼ˆä½ç½®è¶Šé åï¼Œæƒé‡è¶Šé«˜ï¼‰
    ips_weight = 1.0 / propensity

    return ips_weight


def generate_unbiased_training_data(search_history: List[Dict]) -> List[Dict]:
    """
    ç”Ÿæˆå»åçš„è®­ç»ƒæ•°æ®

    å¯¹ä½ç½®é åä½†è´¨é‡é«˜çš„ç»“æœç»™äºˆæ›´é«˜æƒé‡
    """
    training_data = []

    for search in search_history:
        for position, result in enumerate(search['results'], 1):
            pseudo_ctr = position_to_pseudo_ctr(position)

            # è®¡ç®— IPS æƒé‡
            ips_weight = inverse_propensity_scoring(position)

            training_data.append({
                'query': search['query'],
                'conversation': result,
                'user_history': search['context'].get('user_history', {}),
                'context': search['context'],
                'position': position,
                'label': pseudo_ctr,
                'weight': ips_weight,  # å»åæƒé‡
                'is_pseudo': True
            })

    return training_data
```

### å®é™…åº”ç”¨ç­–ç•¥

```python
class ColdStartStrategy:
    """å†·å¯åŠ¨ç­–ç•¥ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""

    def get_model(self, user_history: Dict, search_history: List[Dict]):
        """æ ¹æ®ç”¨æˆ·æ•°æ®é‡é€‰æ‹©ç­–ç•¥"""

        real_feedback_count = len(user_history.get('feedbacks', []))

        if real_feedback_count == 0:
            # é˜¶æ®µ1ï¼šçº¯ä¼ªæ ‡ç­¾è®­ç»ƒ
            print("ğŸ“Š é˜¶æ®µ1ï¼šä½¿ç”¨ä¼ªæ ‡ç­¾è®­ç»ƒåˆå§‹æ¨¡å‹")
            model = LogisticRegressionCTR()
            pseudo_data = generate_pseudo_training_data(search_history)
            model.train(pseudo_data)
            return model

        elif real_feedback_count < 50:
            # é˜¶æ®µ2ï¼šæ··åˆè®­ç»ƒï¼ˆä¼ªæ ‡ç­¾ + çœŸå®åé¦ˆï¼‰
            print(f"ğŸ“Š é˜¶æ®µ2ï¼šæ··åˆè®­ç»ƒ ({real_feedback_count} çœŸå®åé¦ˆ)")
            strategy = HybridTrainingStrategy()
            strategy.initialize(search_history)
            strategy.collect_real_feedback(user_history['feedbacks'])
            return strategy.model

        else:
            # é˜¶æ®µ3ï¼šçº¯çœŸå®åé¦ˆè®­ç»ƒ
            print(f"ğŸ“Š é˜¶æ®µ3ï¼šçº¯çœŸå®åé¦ˆè®­ç»ƒ ({real_feedback_count} æ¡)")
            model = LogisticRegressionCTR()
            model.train(user_history['feedbacks'])
            return model
```

### è¯„ä¼°ï¼šä¼ªæ ‡ç­¾ vs çœŸå®æ ‡ç­¾

```python
async def evaluate_pseudo_labels(search_history: List[Dict],
                                 real_feedback: List[Dict]) -> Dict[str, float]:
    """
    è¯„ä¼°ä¼ªæ ‡ç­¾çš„è´¨é‡

    å¯¹æ¯”ï¼š
    - ä¼ªæ ‡ç­¾é¢„æµ‹çš„ CTR
    - çœŸå®ç”¨æˆ·è¡Œä¸ºçš„ CTR
    """
    results = {
        'pseudo_accuracy': 0.0,
        'position_correlation': 0.0,
        'top3_agreement': 0.0
    }

    # 1. å‡†ç¡®ç‡ï¼šä¼ªæ ‡ç­¾é¢„æµ‹çš„ top-3 å’ŒçœŸå® top-3 çš„é‡å åº¦
    pseudo_top3 = get_pseudo_top3(search_history)
    real_top3 = get_real_top3(real_feedback)
    results['top3_agreement'] = calculate_overlap(pseudo_top3, real_top3)

    # 2. ç›¸å…³æ€§ï¼šä½ç½®å’ŒçœŸå® CTR çš„ç›¸å…³æ€§
    results['position_correlation'] = calculate_correlation(
        [f['position'] for f in real_feedback],
        [f['label'] for f in real_feedback]
    )

    return results
```

### æ€»ç»“ï¼šæ¨èæ–¹æ¡ˆ

**æœ€ä½³å®è·µï¼šæ··åˆè®­ç»ƒç­–ç•¥**

```
é˜¶æ®µ1ï¼ˆ0 çœŸå®åé¦ˆï¼‰ï¼š
  â”œâ”€ ä»å†å²æœç´¢ç”Ÿæˆä¼ªæ ‡ç­¾
  â”œâ”€ è®­ç»ƒåˆå§‹ LR æ¨¡å‹
  â””â”€ ç«‹å³å¯ç”¨

é˜¶æ®µ2ï¼ˆ1-50 çœŸå®åé¦ˆï¼‰ï¼š
  â”œâ”€ æ”¶é›†çœŸå®ç”¨æˆ·åé¦ˆ
  â”œâ”€ æ··åˆè®­ç»ƒï¼ˆçœŸå®æƒé‡é€æ­¥å¢åŠ ï¼‰
  â””â”€ æ¨¡å‹é€æ­¥æ”¹è¿›

é˜¶æ®µ3ï¼ˆ50+ çœŸå®åé¦ˆï¼‰ï¼š
  â”œâ”€ åœç”¨ä¼ªæ ‡ç­¾
  â”œâ”€ çº¯çœŸå®åé¦ˆè®­ç»ƒ
  â””â”€ æŒç»­åœ¨çº¿å­¦ä¹ 
```

**ä¼˜åŠ¿**ï¼š
- âœ… ç«‹å³å¯ç”¨ï¼ˆä¸éœ€è¦ç­‰å¾…æ•°æ®ï¼‰
- âœ… å¹³æ»‘è¿‡æ¸¡ï¼ˆä»ä¼ªæ ‡ç­¾åˆ°çœŸå®åé¦ˆï¼‰
- âœ… æŒç»­æ”¹è¿›ï¼ˆéšç€ä½¿ç”¨è¶Šæ¥è¶Šå‡†ç¡®ï¼‰
- âœ… é¿å…å†·å¯åŠ¨é—®é¢˜

## æ€»ç»“

### æ ¸å¿ƒè®¾è®¡

1. **ç‰¹å¾å·¥ç¨‹**ï¼š6 ç±»ç‰¹å¾ï¼Œ50+ ç»´åº¦
2. **æ¨¡å‹é€‰æ‹©**ï¼šLogistic Regressionï¼ˆç®€å•ã€å¿«é€Ÿã€å¯è§£é‡Šï¼‰
3. **è®­ç»ƒæ•°æ®**ï¼šéšå¼åé¦ˆè‡ªåŠ¨æ ‡æ³¨
4. **åœ¨çº¿å­¦ä¹ **ï¼šå®æ—¶æ›´æ–°æ¨¡å‹
5. **å†·å¯åŠ¨**ï¼šå¯å‘å¼æƒé‡ â†’ åœ¨çº¿å­¦ä¹ 

### ä¼˜å…ˆçº§

1. **Phase 6.1** - ç‰¹å¾å·¥ç¨‹ + åé¦ˆæ”¶é›† â­â­â­â­â­
2. **Phase 6.2** - LR æ¨¡å‹ + å¯å‘å¼æƒé‡ â­â­â­â­â­
3. **Phase 6.3** - åœ¨çº¿å­¦ä¹  â­â­â­â­
4. **Phase 6.4** - é«˜çº§æ¨¡å‹ï¼ˆFMï¼‰â­â­â­

### é¢„æœŸæ•ˆæœ

- æœç´¢ç»“æœæ’åºæ›´å‡†ç¡®
- ç”¨æˆ·éœ€è¦çš„ç»“æœæ’åœ¨å‰é¢
- éšç€ä½¿ç”¨è¶Šæ¥è¶Šæ™ºèƒ½
- å¹³å‡é€‰æ‹©ä½ç½®ä» 3-4 é™åˆ° 1-2

**æœ€å¤§ä»·å€¼**ï¼šä»"é™æ€æ’åº"å‡çº§åˆ°"é¢„æµ‹å¼æ’åº"ã€‚
