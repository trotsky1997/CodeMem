#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Clustering and aggregation module for Phase 4.5.

Features:
- Query clustering (similar queries)
- Topic aggregation (hierarchical topics)
- Session clustering (conversation types)
- Problem pattern recognition (recurring issues)
"""

import re
from typing import List, Dict, Any, Tuple, Set
from collections import defaultdict, Counter
from difflib import SequenceMatcher


class PatternClusterer:
    """
    Clusters and aggregates patterns from user behavior.
    """

    def __init__(self, events: List[Dict[str, Any]]):
        """
        Initialize pattern clusterer.

        Args:
            events: List of event dictionaries
        """
        self.events = events
        self.user_messages = [e for e in events if e.get("role") == "user"]

    def cluster_queries(self, similarity_threshold: float = 0.6) -> List[Dict[str, Any]]:
        """
        Cluster similar queries together.

        Args:
            similarity_threshold: Minimum similarity to group queries (0-1)

        Returns:
            List of query clusters
        """
        if not self.user_messages:
            return []

        # Extract query texts
        queries = [msg.get("text", "") for msg in self.user_messages]

        # Build clusters
        clusters = []
        used_indices = set()

        for i, query1 in enumerate(queries):
            if i in used_indices:
                continue

            # Start new cluster
            cluster = {
                "representative": query1,
                "queries": [query1],
                "count": 1,
                "sessions": [self.user_messages[i].get("session_id")]
            }

            # Find similar queries
            for j, query2 in enumerate(queries):
                if j <= i or j in used_indices:
                    continue

                similarity = self._calculate_similarity(query1, query2)
                if similarity >= similarity_threshold:
                    cluster["queries"].append(query2)
                    cluster["count"] += 1
                    cluster["sessions"].append(self.user_messages[j].get("session_id"))
                    used_indices.add(j)

            used_indices.add(i)
            clusters.append(cluster)

        # Sort by count
        clusters.sort(key=lambda x: x["count"], reverse=True)

        # Only return clusters with more than 1 query
        return [c for c in clusters if c["count"] > 1]

    def aggregate_topics(self) -> Dict[str, Any]:
        """
        Aggregate fine-grained topics into hierarchical structure.

        Returns:
            Dict with topic hierarchy
        """
        # Define topic hierarchy
        topic_hierarchy = {
            "ç¼–ç¨‹è¯­è¨€": {
                "keywords": ["python", "javascript", "java", "go", "rust", "ruby"],
                "subtopics": {}
            },
            "å¼‚æ­¥ç¼–ç¨‹": {
                "keywords": ["async", "asyncio", "å¼‚æ­¥", "åç¨‹", "concurrent", "å¹¶å‘"],
                "subtopics": {}
            },
            "æ•°æ®å­˜å‚¨": {
                "keywords": ["database", "æ•°æ®åº“", "sql", "nosql", "cache", "ç¼“å­˜", "redis"],
                "subtopics": {}
            },
            "æ€§èƒ½ä¼˜åŒ–": {
                "keywords": ["performance", "æ€§èƒ½", "optimization", "ä¼˜åŒ–", "é€Ÿåº¦", "æ•ˆç‡"],
                "subtopics": {}
            },
            "æµ‹è¯•è°ƒè¯•": {
                "keywords": ["test", "æµ‹è¯•", "debug", "è°ƒè¯•", "error", "é”™è¯¯", "bug"],
                "subtopics": {}
            },
            "APIå¼€å‘": {
                "keywords": ["api", "rest", "http", "æ¥å£", "endpoint"],
                "subtopics": {}
            },
            "æ¶æ„è®¾è®¡": {
                "keywords": ["architecture", "æ¶æ„", "design", "è®¾è®¡", "pattern", "æ¨¡å¼"],
                "subtopics": {}
            }
        }

        # Count occurrences for each topic
        for topic_name, topic_info in topic_hierarchy.items():
            count = 0
            examples = []

            for event in self.events:
                text = event.get("text", "").lower()
                if any(kw in text for kw in topic_info["keywords"]):
                    count += 1
                    if len(examples) < 3:
                        examples.append(text[:100])

            topic_info["count"] = count
            topic_info["examples"] = examples

        # Sort by count
        sorted_topics = sorted(
            topic_hierarchy.items(),
            key=lambda x: x[1]["count"],
            reverse=True
        )

        return {
            "hierarchy": dict(sorted_topics),
            "total_topics": len([t for t in topic_hierarchy.values() if t.get("count", 0) > 0])
        }

    def cluster_sessions(self) -> List[Dict[str, Any]]:
        """
        Cluster sessions by conversation type.

        Returns:
            List of session clusters
        """
        # Group events by session
        sessions = defaultdict(list)
        for event in self.events:
            session_id = event.get("session_id")
            if session_id:
                sessions[session_id].append(event)

        # Classify each session
        session_types = {
            "å­¦ä¹ å‹": [],
            "é—®é¢˜è§£å†³å‹": [],
            "æ¢ç´¢å‹": [],
            "å®è·µå‹": []
        }

        for session_id, events in sessions.items():
            session_type = self._classify_session(events)
            session_types[session_type].append({
                "session_id": session_id,
                "event_count": len(events),
                "sample": events[0].get("text", "")[:100] if events else ""
            })

        # Format results
        clusters = []
        for type_name, sessions_list in session_types.items():
            if sessions_list:
                clusters.append({
                    "type": type_name,
                    "count": len(sessions_list),
                    "sessions": sessions_list[:5]  # Top 5 examples
                })

        return sorted(clusters, key=lambda x: x["count"], reverse=True)

    def recognize_problem_patterns(self) -> List[Dict[str, Any]]:
        """
        Recognize recurring problem patterns.

        Returns:
            List of problem patterns
        """
        # Extract problem-related messages
        problem_keywords = [
            "é”™è¯¯", "error", "bug", "é—®é¢˜", "issue",
            "ä¸å·¥ä½œ", "doesn't work", "å¤±è´¥", "failed",
            "æŠ¥é”™", "exception", "å¼‚å¸¸"
        ]

        problem_messages = []
        for event in self.user_messages:
            text = event.get("text", "").lower()
            if any(kw in text for kw in problem_keywords):
                problem_messages.append(event)

        if not problem_messages:
            return []

        # Cluster similar problems
        problem_clusters = []
        used_indices = set()

        for i, msg1 in enumerate(problem_messages):
            if i in used_indices:
                continue

            text1 = msg1.get("text", "")
            cluster = {
                "pattern": self._extract_problem_pattern(text1),
                "occurrences": [text1],
                "count": 1,
                "sessions": [msg1.get("session_id")]
            }

            # Find similar problems
            for j, msg2 in enumerate(problem_messages):
                if j <= i or j in used_indices:
                    continue

                text2 = msg2.get("text", "")
                if self._are_similar_problems(text1, text2):
                    cluster["occurrences"].append(text2)
                    cluster["count"] += 1
                    cluster["sessions"].append(msg2.get("session_id"))
                    used_indices.add(j)

            used_indices.add(i)
            if cluster["count"] > 1:  # Only recurring patterns
                problem_clusters.append(cluster)

        return sorted(problem_clusters, key=lambda x: x["count"], reverse=True)

    def generate_aggregation_report(self) -> Dict[str, Any]:
        """
        Generate comprehensive aggregation report.

        Returns:
            Dict with all aggregation results
        """
        query_clusters = self.cluster_queries(similarity_threshold=0.6)
        topic_aggregation = self.aggregate_topics()
        session_clusters = self.cluster_sessions()
        problem_patterns = self.recognize_problem_patterns()

        # Generate summary
        summary_parts = []

        if query_clusters:
            summary_parts.append(f"å‘ç° {len(query_clusters)} ç»„ç›¸ä¼¼æŸ¥è¯¢")

        if topic_aggregation["total_topics"] > 0:
            summary_parts.append(f"æ¶µç›– {topic_aggregation['total_topics']} ä¸ªä¸»é¢˜é¢†åŸŸ")

        if session_clusters:
            top_session_type = session_clusters[0]
            summary_parts.append(f"ä¸»è¦æ˜¯{top_session_type['type']}ä¼šè¯ ({top_session_type['count']}æ¬¡)")

        if problem_patterns:
            summary_parts.append(f"è¯†åˆ«å‡º {len(problem_patterns)} ç§é‡å¤é—®é¢˜æ¨¡å¼")

        summary = "ã€‚".join(summary_parts) + "ã€‚" if summary_parts else "æš‚æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œèšç±»åˆ†æã€‚"

        return {
            "summary": summary,
            "query_clusters": query_clusters[:10],  # Top 10
            "topic_aggregation": topic_aggregation,
            "session_clusters": session_clusters,
            "problem_patterns": problem_patterns[:5]  # Top 5
        }

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate similarity between two texts."""
        # Normalize
        text1 = text1.lower().strip()
        text2 = text2.lower().strip()

        # Use SequenceMatcher for similarity
        return SequenceMatcher(None, text1, text2).ratio()

    def _classify_session(self, events: List[Dict[str, Any]]) -> str:
        """Classify session type based on content."""
        user_texts = [e.get("text", "").lower() for e in events if e.get("role") == "user"]
        combined_text = " ".join(user_texts)

        # Learning indicators
        learning_keywords = ["ä»€ä¹ˆæ˜¯", "å¦‚ä½•", "æ€ä¹ˆ", "ä»‹ç»", "æ•™ç¨‹", "å­¦ä¹ ", "what is", "how to", "tutorial"]
        if any(kw in combined_text for kw in learning_keywords):
            return "å­¦ä¹ å‹"

        # Problem-solving indicators
        problem_keywords = ["é”™è¯¯", "bug", "é—®é¢˜", "ä¸å·¥ä½œ", "æŠ¥é”™", "error", "issue", "doesn't work"]
        if any(kw in combined_text for kw in problem_keywords):
            return "é—®é¢˜è§£å†³å‹"

        # Practice indicators
        practice_keywords = ["å®ç°", "å†™", "åˆ›å»º", "å¼€å‘", "implement", "create", "develop", "build"]
        if any(kw in combined_text for kw in practice_keywords):
            return "å®è·µå‹"

        # Default to exploration
        return "æ¢ç´¢å‹"

    def _extract_problem_pattern(self, text: str) -> str:
        """Extract problem pattern from text."""
        # Remove specific details, keep general pattern
        text = text.lower()

        # Replace specific values with placeholders
        text = re.sub(r'\d+', 'N', text)  # Numbers
        text = re.sub(r'["\'].*?["\']', 'STRING', text)  # Strings

        # Extract key phrases
        if "error" in text or "é”™è¯¯" in text:
            return "é”™è¯¯/å¼‚å¸¸é—®é¢˜"
        elif "slow" in text or "æ…¢" in text or "performance" in text or "æ€§èƒ½" in text:
            return "æ€§èƒ½é—®é¢˜"
        elif "not work" in text or "ä¸å·¥ä½œ" in text or "failed" in text or "å¤±è´¥" in text:
            return "åŠŸèƒ½å¤±æ•ˆé—®é¢˜"
        elif "how to" in text or "å¦‚ä½•" in text or "æ€ä¹ˆ" in text:
            return "ä½¿ç”¨æ–¹æ³•é—®é¢˜"
        else:
            return "ä¸€èˆ¬é—®é¢˜"

    def _are_similar_problems(self, text1: str, text2: str) -> bool:
        """Check if two problems are similar."""
        pattern1 = self._extract_problem_pattern(text1)
        pattern2 = self._extract_problem_pattern(text2)

        # Same pattern type
        if pattern1 == pattern2:
            # Also check text similarity
            similarity = self._calculate_similarity(text1, text2)
            return similarity > 0.4

        return False


def format_aggregation_report(report: Dict[str, Any]) -> str:
    """
    Format aggregation report as readable text.

    Args:
        report: Aggregation report dict

    Returns:
        Formatted text report
    """
    lines = []

    lines.append("# ğŸ” æ¨¡å¼èšç±»åˆ†ææŠ¥å‘Š\n")
    lines.append(f"## ğŸ“ æ‘˜è¦\n")
    lines.append(f"{report['summary']}\n")

    # Query clusters
    if report["query_clusters"]:
        lines.append(f"## ğŸ”— ç›¸ä¼¼æŸ¥è¯¢èšç±»\n")
        for i, cluster in enumerate(report["query_clusters"][:5], 1):
            lines.append(f"{i}. **{cluster['representative'][:80]}** ({cluster['count']} æ¬¡)")
            if len(cluster['queries']) > 1:
                lines.append(f"   ç›¸ä¼¼æŸ¥è¯¢: {cluster['queries'][1][:60]}...")
        lines.append("")

    # Topic aggregation
    if report["topic_aggregation"]["total_topics"] > 0:
        lines.append(f"## ğŸ“š è¯é¢˜èšåˆ\n")
        hierarchy = report["topic_aggregation"]["hierarchy"]
        for topic_name, topic_info in list(hierarchy.items())[:5]:
            if topic_info.get("count", 0) > 0:
                lines.append(f"- **{topic_name}**: {topic_info['count']} æ¬¡")
        lines.append("")

    # Session clusters
    if report["session_clusters"]:
        lines.append(f"## ğŸ’¬ ä¼šè¯ç±»å‹åˆ†å¸ƒ\n")
        for cluster in report["session_clusters"]:
            lines.append(f"- **{cluster['type']}**: {cluster['count']} æ¬¡ä¼šè¯")
        lines.append("")

    # Problem patterns
    if report["problem_patterns"]:
        lines.append(f"## âš ï¸ é‡å¤é—®é¢˜æ¨¡å¼\n")
        for i, pattern in enumerate(report["problem_patterns"], 1):
            lines.append(f"{i}. **{pattern['pattern']}** (å‡ºç° {pattern['count']} æ¬¡)")
            lines.append(f"   ç¤ºä¾‹: {pattern['occurrences'][0][:80]}...")
        lines.append("")

    return "\n".join(lines)
